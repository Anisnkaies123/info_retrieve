{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting GoldenRetriever to TF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USE-QA in TF2\n",
    "https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\n",
    "Official code sample for TF2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons\n",
    "# !pip install tensorflow_text\n",
    "# !pip install --upgrade tensorflow-hub\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "\n",
    "module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "\n",
    "question_embeddings = module.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "response_embeddings = module.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "\n",
    "np.inner(question_embeddings['outputs'], response_embeddings['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Golden Retriever in TF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tensorboard/graphs\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "from utils import split_txt, read_txt, clean_txt, read_kb_csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def triplet_loss(anchor_vector, positive_vector, negative_vector, metric='cosine_dist', margin=0.009):\n",
    "    \"\"\"Computes the triplet loss with semi-hard negative mining.\n",
    "    The loss encourages the positive distances (between a pair of embeddings with\n",
    "    the same labels) to be smaller than the minimum negative distance among\n",
    "    which are at least greater than the positive distance plus the margin constant\n",
    "    (called semi-hard negative) in the mini-batch. If no such negative exists,\n",
    "    uses the largest negative distance instead.\n",
    "    See: https://arxiv.org/abs/1503.03832.\n",
    "\n",
    "    Args:\n",
    "        labels: 1-D tf.int32 `Tensor` with shape [batch_size] of\n",
    "        multiclass integer labels.\n",
    "        embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
    "        be l2 normalized.\n",
    "        metric: 'cosine_dist' (default)\n",
    "        margin: Float, margin term in the loss definition. default based on https://arxiv.org/pdf/1508.01585.pdf\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: tf.float32 scalar.\n",
    "    \"\"\"\n",
    "    cosine_distance = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "    d_pos = cosine_distance(anchor_vector, positive_vector)\n",
    "    d_neg = cosine_distance(anchor_vector, negative_vector)\n",
    "    loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "class GoldenRetriever:\n",
    "    \"\"\"GoldenRetriever model for information retrieval prediction and finetuning.\n",
    "    Parameters\n",
    "    ----------\n",
    "    margin: margin to be used if loss='triplet' (default 0.3)\n",
    "    loss: loss function to use. Options are 'cosine', 'contrastive', or 'triplet'(default) which is a triplet loss based on cosine distance.\n",
    "    **kwargs: keyword arguments for Adam() optimizer\n",
    "\n",
    "    Example:\n",
    "    >>> gr = GoldenRetriever()\n",
    "    >>> text_list = ['I love my chew toy!', 'I hate Mondays.']\n",
    "    >>> gr.load_kb(text_list=text_list)\n",
    "    >>> gr.make_query('what do you not love?', top_k=1)\n",
    "    ['I hate Mondays.']\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, margin=0.3, loss='triplet', **kwargs):\n",
    "        # self.v=['QA/Final/Response_tuning/ResidualHidden_1/dense/kernel','QA/Final/Response_tuning/ResidualHidden_0/dense/kernel', 'QA/Final/Response_tuning/ResidualHidden_1/AdjustDepth/projection/kernel']\n",
    "        self.v=['QA/Final/Response_tuning/ResidualHidden_1/dense/kernel']\n",
    "        self.margin = margin\n",
    "        self.loss = loss\n",
    "        self.vectorized_knowledge = {}\n",
    "        self.text = {}\n",
    "        self.questions = {}\n",
    "\n",
    "        # init saved model\n",
    "        self.embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/2')\n",
    "        self.question_encoder = self.embed.signatures['question_encoder']\n",
    "        self.response_encoder = self.embed.signatures['response_encoder']\n",
    "        self.neg_response_encoder = self.embed.signatures['response_encoder']\n",
    "        print('model initiated!')\n",
    "        \n",
    "        # optimizer & losses\n",
    "        self.optimizer = tf.keras.optimizers.Adam(**kwargs)\n",
    "        self.cost_history = []\n",
    "        self.var_finetune=[x for x in self.embed.variables for vv in self.v if vv in x.name] #get the weights we want to finetune.\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, text, context=None, type='response'):\n",
    "        \"\"\"Return the tensor representing embedding of input text.\n",
    "        Type can be 'query' or 'response' \"\"\"\n",
    "        if type=='query':\n",
    "            return self.question_encoder(tf.constant([text]))['outputs']\n",
    "            # return self.session.run(self.question_embeddings, feed_dict={self.question:text})['outputs']\n",
    "        elif type=='response':\n",
    "            if not context:\n",
    "                context = text\n",
    "            return self.response_encoder(input=tf.constant(text),\n",
    "                                         context=tf.constant(context))['outputs']\n",
    "        else: print('Type of prediction not defined')\n",
    "        \n",
    "    def make_query(self, querystring, top_k=5, index=False, predict_type='query', kb_name='default_kb'):\n",
    "        \"\"\"Make a query against the stored vectorized knowledge. \n",
    "        Choose index=True to return sorted index of matches.\n",
    "        type can be 'query' or 'response' if you are comparing statements\n",
    "        \"\"\"\n",
    "        similarity_score=cosine_similarity(self.vectorized_knowledge[kb_name], self.predict([querystring], type=predict_type))\n",
    "        sortargs=np.flip(similarity_score.argsort(axis=0))\n",
    "        sortargs=[x[0] for x in sortargs]\n",
    "        sorted_ans=[self.text[kb_name][i] for i in sortargs]\n",
    "        if index:\n",
    "            return sorted_ans[:top_k], sortargs[:top_k]\n",
    "        return sorted_ans[:top_k], similarity_score[sortargs[:top_k]] \n",
    "        \n",
    "        \n",
    "    def finetune(self, question, answer, context, neg_answer=[], neg_answer_context=[], label=[]):\n",
    "        \"\"\"\n",
    "        Apply gradients on\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # https://www.tensorflow.org/guide/eager\n",
    "\n",
    "            # get encodings\n",
    "            question_embeddings = self.question_encoder(tf.constant(question))['outputs']\n",
    "            response_embeddings = self.response_encoder(input=tf.constant(answer), \n",
    "                                                        context=tf.constant(context))['outputs']\n",
    "\n",
    "            if self.loss == 'cosine':\n",
    "                \"\"\"\n",
    "                # https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\n",
    "                \"\"\"\n",
    "                self.cost = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "                cost_value = self.cost(question_embeddings, response_embeddings)\n",
    "                \n",
    "            elif self.loss == 'contrastive':\n",
    "                \"\"\"\n",
    "                https://www.tensorflow.org/addons/api_docs/python/tfa/losses/ContrastiveLoss\n",
    "                \n",
    "                y_true to be a vector of binary labels\n",
    "                y_hat to be the respective distances\n",
    "                \"\"\"\n",
    "                self.cosine_dist = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "                cosine_dist_value = self.cosine_dist(question_embeddings, response_embeddings)\n",
    "                \n",
    "                self.cost = tfa.losses.contrastive.ContrastiveLoss(margin = self.margin)\n",
    "                cost_value = self.cost(label, cosine_dist_value)\n",
    "                \n",
    "            elif self.loss == 'triplet':\n",
    "                \"\"\"\n",
    "                https://www.tensorflow.org/addons/tutorials/losses_triplet\n",
    "                \"\"\"\n",
    "                neg_response_embeddings = self.neg_response_encoder(input=tf.constant(neg_answer), \n",
    "                                                                    context=tf.constant(neg_answer_context))['outputs']\n",
    "                self.cost = tfa.losses.TripletSemiHardLoss(margin = self.margin)\n",
    "                cost_value = triplet_loss(question_embeddings, response_embeddings, neg_response_embeddings)\n",
    "\n",
    "                \n",
    "        # record loss     \n",
    "        self.cost_history.append(cost_value.numpy().mean())\n",
    "        \n",
    "        # apply gradient\n",
    "        grads = tape.gradient(cost_value, self.var_finetune)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.var_finetune))\n",
    "\n",
    "        return cost_value.numpy().mean()\n",
    "        \n",
    "    def load_kb(self, path_to_kb=None, text_list=None, question_list=None, \n",
    "                raw_text=None, is_faq=False, kb_name='default_kb'):\n",
    "        r\"\"\"Give either path to .txt document or list of clauses.\n",
    "        For text document, each clause is separated by 2 newlines ('\\\\n\\\\n')\"\"\"\n",
    "        if text_list:\n",
    "            self.text[kb_name] = text_list\n",
    "            if is_faq:\n",
    "                self.questions[kb_name] = question_list\n",
    "        elif path_to_kb:\n",
    "            if is_faq:\n",
    "                self.text[kb_name], self.questions[kb_name] = split_txt(read_txt(path_to_kb), is_faq)\n",
    "            else:\n",
    "                self.text[kb_name] = split_txt(read_txt(path_to_kb), is_faq)\n",
    "        elif raw_text:\n",
    "            delim = '\\n'\n",
    "            self.text[kb_name] = split_txt([front+delim for front in raw_text.split('\\n')])\n",
    "        else: raise NameError('invalid kb input!')\n",
    "        self.vectorized_knowledge[kb_name] = self.predict(clean_txt(self.text[kb_name]), type='response')\n",
    "        print('knowledge base lock and loaded!')\n",
    "        \n",
    "    def load_csv_kb(self, path_to_kb=None, kb_name='default_kb', meta_col='meta', answer_col='answer', \n",
    "                    query_col='question', answer_str_col='answer', cutoff=None):\n",
    "        self.text[kb_name], self.questions[kb_name] = read_kb_csv(path_to_kb, meta_col=meta_col, answer_col=answer_col, \n",
    "                            query_col=query_col, answer_str_col=answer_str_col, cutoff=None)\n",
    "        self.vectorized_knowledge[kb_name] = self.predict(clean_txt(self.text[kb_name]), type='response')\n",
    "        print('knowledge base (csv) lock and loaded!')\n",
    "        \n",
    "    def export(self, savepath='fine_tuned_{}'.format(datetime.datetime.now())):\n",
    "        '''Path should include partial filename.'''\n",
    "        tf.saved_model.save(self.embed, 'fine_tuned')\n",
    "\n",
    "    def restore(self, savepath):\n",
    "        self.embed = tf.saved_model.load(savepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Testing key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../src')\n",
    "\n",
    "# %time gr = GoldenRetriever()\n",
    "\n",
    "# # encode 1 question\n",
    "# encoded_ques = gr.predict('How old are you?', \n",
    "#                           type='query')\n",
    "\n",
    "# # encode multiple questions\n",
    "# encoded_ques = gr.predict(['How old are you?', 'What time is it?'], \n",
    "#                           type='query')\n",
    "\n",
    "# # one response w context\n",
    "# encoded_res = gr.predict(\"I am 20 years old.\", \n",
    "#                          context=\"I will be 21 next year.\", \n",
    "#                          type='response')\n",
    "\n",
    "# # multiple responses w/0 context\n",
    "# encoded_res = gr.predict([\"I am 20 years old.\", \"I love apple cider\"], \n",
    "#                          type='response')\n",
    "\n",
    "# # load knowledge bases\n",
    "# gr.load_kb(path_to_kb='../data/aiap.txt', is_faq=True, kb_name='aiap')\n",
    "# # gr.load_kb(path_to_kb='./data/resale_tnc.txt', kb_name='resale_tnc')\n",
    "# # gr.load_kb(path_to_kb='./data/fund_guide_tnc_full.txt', kb_name='nrf')\n",
    "# # gr.load_csv_kb(path_to_kb='./data/pdpa.csv', cutoff=196, kb_name='pdpa')\n",
    "\n",
    "# # make query\n",
    "# gr.make_query('What kind of candidates are you looking for?', top_k=2, kb_name='aiap')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# 1. Cosine loss\n",
    "# \"\"\"\n",
    "# gr = GoldenRetriever(loss='cosine')\n",
    "\n",
    "# print(\"BEFORE BACKPROP\")\n",
    "# print(gr.var_finetune)\n",
    "# print(\"\")\n",
    "\n",
    "# questions = [\"What is your age?\"]\n",
    "# responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "# response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "# gr.finetune(questions, responses, response_contexts, label=[1,0])\n",
    "\n",
    "# print(\"AFTER BACKPROP\")\n",
    "# print(gr.var_finetune)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# 2. Contrastive loss\n",
    "# \"\"\"\n",
    "# gr = GoldenRetriever(loss='contrastive')\n",
    "\n",
    "# print(\"BEFORE BACKPROP\")\n",
    "# print(gr.var_finetune)\n",
    "# print(\"\")\n",
    "\n",
    "# questions = [\"What is your age?\"]\n",
    "# responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "# response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "# gr.finetune(questions, responses, response_contexts, label=[1,0])\n",
    "\n",
    "# print(\"AFTER BACKPROP\")\n",
    "# print(gr.var_finetune)\n",
    "\n",
    "# \"\"\"\n",
    "# 3. Triplet loss\n",
    "# \"\"\"\n",
    "# gr = GoldenRetriever(loss='triplet')\n",
    "\n",
    "# print(\"BEFORE BACKPROP\")\n",
    "# print(gr.var_finetune)\n",
    "# print(\"\")\n",
    "\n",
    "# questions = [\"What is your age?\"]\n",
    "# responses = [\"The top section of the spine is damaged.\"]\n",
    "# response_contexts = [\"Call the nurse.\"]\n",
    "# %time gr.finetune(questions, responses, response_contexts, neg_answer = [\"I will be 21 years old.\"], neg_answer_context = [\"Time is running out for the elderly and the young.\"])\n",
    "\n",
    "# print(\"AFTER BACKPROP\")\n",
    "# print(gr.var_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few values in the array has changed, indicating that there is successful tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing src saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timing init\n",
      "model initiated!\n",
      "CPU times: user 19.2 s, sys: 1.88 s, total: 21.1 s\n",
      "Wall time: 30.7 s\n",
      "\n",
      "timing ques encoding\n",
      "CPU times: user 2.04 s, sys: 66.8 ms, total: 2.11 s\n",
      "Wall time: 2.12 s\n",
      "\n",
      "timing response encoding\n",
      "CPU times: user 2.25 s, sys: 13.4 ms, total: 2.26 s\n",
      "Wall time: 2.24 s\n",
      "\n",
      "knowledge base lock and loaded!\n",
      "model initiated!\n",
      "BEFORE BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00305502,  0.00504641,  0.01790689, ..., -0.02388328,\n",
      "         0.03720526,  0.04548807],\n",
      "       [ 0.04789947, -0.02582268,  0.08293641, ...,  0.0698828 ,\n",
      "        -0.04037469, -0.02779369],\n",
      "       [-0.05143448, -0.06723368,  0.02879738, ..., -0.04495105,\n",
      "        -0.04067428, -0.01053122]], dtype=float32)>]\n",
      "\n",
      "CPU times: user 29.7 s, sys: 230 ms, total: 29.9 s\n",
      "Wall time: 29.9 s\n",
      "AFTER BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00206157,  0.00408014,  0.01691419, ..., -0.02288993,\n",
      "         0.03819802,  0.04456301],\n",
      "       [ 0.04889183, -0.02678634,  0.08194487, ...,  0.07087511,\n",
      "        -0.03938333, -0.02870969],\n",
      "       [-0.05051216, -0.06792539,  0.02788324, ..., -0.04402984,\n",
      "        -0.03975946, -0.01102271]], dtype=float32)>]\n",
      "model initiated!\n",
      "BEFORE BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00305502,  0.00504641,  0.01790689, ..., -0.02388328,\n",
      "         0.03720526,  0.04548807],\n",
      "       [ 0.04789947, -0.02582268,  0.08293641, ...,  0.0698828 ,\n",
      "        -0.04037469, -0.02779369],\n",
      "       [-0.05143448, -0.06723368,  0.02879738, ..., -0.04495105,\n",
      "        -0.04067428, -0.01053122]], dtype=float32)>]\n",
      "\n",
      "CPU times: user 28.2 s, sys: 147 ms, total: 28.3 s\n",
      "Wall time: 28.2 s\n",
      "AFTER BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00404682,  0.00600449,  0.01889775, ..., -0.02487496,\n",
      "         0.03621432,  0.04639587],\n",
      "       [ 0.04690903, -0.02486782,  0.08392584, ...,  0.06889242,\n",
      "        -0.04136387, -0.02689682],\n",
      "       [-0.05233898, -0.06659216,  0.02969204, ..., -0.04585421,\n",
      "        -0.04156975, -0.01009589]], dtype=float32)>]\n",
      "model initiated!\n",
      "BEFORE BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00305502,  0.00504641,  0.01790689, ..., -0.02388328,\n",
      "         0.03720526,  0.04548807],\n",
      "       [ 0.04789947, -0.02582268,  0.08293641, ...,  0.0698828 ,\n",
      "        -0.04037469, -0.02779369],\n",
      "       [-0.05143448, -0.06723368,  0.02879738, ..., -0.04495105,\n",
      "        -0.04067428, -0.01053122]], dtype=float32)>]\n",
      "\n",
      "CPU times: user 29.3 s, sys: 154 ms, total: 29.4 s\n",
      "Wall time: 29.1 s\n",
      "AFTER BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00305502,  0.00504641,  0.01790689, ..., -0.02388328,\n",
      "         0.03720526,  0.04548807],\n",
      "       [ 0.04789947, -0.02582268,  0.08293641, ...,  0.0698828 ,\n",
      "        -0.04037469, -0.02779369],\n",
      "       [-0.05143448, -0.06723368,  0.02879738, ..., -0.04495105,\n",
      "        -0.04067428, -0.01053122]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model import GoldenRetriever\n",
    "print(\"timing init\")\n",
    "%time gr = GoldenRetriever({'learning_rate':0.001, 'beta_1':0.9, 'beta_2':0.999})\n",
    "print(\"\")\n",
    "\n",
    "# encode 1 question\n",
    "print(\"timing ques encoding\")\n",
    "%time encoded_ques = gr.predict('How old are you?', type='query')\n",
    "print(\"\")\n",
    "\n",
    "# encode multiple questions\n",
    "encoded_ques = gr.predict(['How old are you?', 'What time is it?'], \n",
    "                          type='query')\n",
    "\n",
    "# one response w context\n",
    "print(\"timing response encoding\")\n",
    "%time encoded_res = gr.predict(\"I am 20 years old.\", context=\"I will be 21 next year.\", type='response')\n",
    "print(\"\")\n",
    "\n",
    "# multiple responses w/0 context\n",
    "encoded_res = gr.predict([\"I am 20 years old.\", \"I love apple cider\"], type='response')\n",
    "\n",
    "# load knowledge bases\n",
    "gr.load_kb(path_to_kb='../data/aiap.txt', is_faq=True, kb_name='aiap')\n",
    "# gr.load_kb(path_to_kb='./data/resale_tnc.txt', kb_name='resale_tnc')\n",
    "# gr.load_kb(path_to_kb='./data/fund_guide_tnc_full.txt', kb_name='nrf')\n",
    "# gr.load_csv_kb(path_to_kb='./data/pdpa.csv', cutoff=196, kb_name='pdpa')\n",
    "\n",
    "# make query\n",
    "gr.make_query('What kind of candidates are you looking for?', top_k=2, kb_name='aiap')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Testing loss functions\n",
    "\"\"\"\n",
    "v=['QA/Final/Response_tuning/ResidualHidden_1/dense/kernel']\n",
    "var_finetune=[x for x in gr.embed.variables for vv in v if vv in x.name] #get the weights we want to finetune.\n",
    "\n",
    "\"\"\"\n",
    "1. Cosine loss\n",
    "\"\"\"\n",
    "gr = GoldenRetriever(loss='cosine')\n",
    "\n",
    "print(\"BEFORE BACKPROP\")\n",
    "print(gr.var_finetune)\n",
    "print(\"\")\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "%time gr.finetune(questions, responses, response_contexts, label=[1,0])\n",
    "\n",
    "print(\"AFTER BACKPROP\")\n",
    "print(gr.var_finetune)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. Contrastive loss\n",
    "\"\"\"\n",
    "gr = GoldenRetriever(loss='contrastive')\n",
    "\n",
    "print(\"BEFORE BACKPROP\")\n",
    "print(gr.var_finetune)\n",
    "print(\"\")\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "%time gr.finetune(questions, responses, response_contexts, label=[1,0])\n",
    "\n",
    "print(\"AFTER BACKPROP\")\n",
    "print(gr.var_finetune)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. Triplet loss\n",
    "\"\"\"\n",
    "gr = GoldenRetriever(loss='triplet')\n",
    "\n",
    "print(\"BEFORE BACKPROP\")\n",
    "print(gr.var_finetune)\n",
    "print(\"\")\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"The top section of the spine is damaged.\"]\n",
    "response_contexts = [\"Call the nurse.\"]\n",
    "%time gr.finetune(questions, responses, response_contexts, neg_answer = [\"I will be 21 years old.\"], neg_answer_context = [\"Time is running out for the elderly and the young.\"])\n",
    "\n",
    "print(\"AFTER BACKPROP\")\n",
    "print(gr.var_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initiated!\n",
      "BEFORE BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00305502,  0.00504641,  0.01790689, ..., -0.02388328,\n",
      "         0.03720526,  0.04548807],\n",
      "       [ 0.04789947, -0.02582268,  0.08293641, ...,  0.0698828 ,\n",
      "        -0.04037469, -0.02779369],\n",
      "       [-0.05143448, -0.06723368,  0.02879738, ..., -0.04495105,\n",
      "        -0.04067428, -0.01053122]], dtype=float32)>]\n",
      "\n",
      "CPU times: user 27.9 s, sys: 165 ms, total: 28 s\n",
      "Wall time: 27.8 s\n",
      "AFTER BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00305502,  0.00504641,  0.01790689, ..., -0.02388328,\n",
      "         0.03720526,  0.04548807],\n",
      "       [ 0.04789947, -0.02582268,  0.08293641, ...,  0.0698828 ,\n",
      "        -0.04037469, -0.02779369],\n",
      "       [-0.05143448, -0.06723368,  0.02879738, ..., -0.04495105,\n",
      "        -0.04067428, -0.01053122]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "3. Triplet loss\n",
    "\"\"\"\n",
    "gr = GoldenRetriever(loss='triplet')\n",
    "\n",
    "print(\"BEFORE BACKPROP\")\n",
    "print(gr.var_finetune)\n",
    "print(\"\")\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\"]\n",
    "response_contexts = [\"I will be 21 next year.\"]\n",
    "%time gr.finetune(questions, responses, response_contexts, neg_answer = [\"good morning\"], neg_answer_context = [\"great day.\"])\n",
    "\n",
    "print(\"AFTER BACKPROP\")\n",
    "print(gr.var_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "\n",
    "# load the module: v3 does not support finetuning? but v2 is fine\n",
    "module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/2')\n",
    "question_encoder = module.signatures['question_encoder']\n",
    "response_encoder = module.signatures['response_encoder']\n",
    "        \n",
    "# get trainable layers\n",
    "#v=['QA/Final/Response_tuning/ResidualHidden_1/AdjustDepth/projection/kernel']\n",
    "v=['QA/Final/Response_tuning/ResidualHidden_1/dense/kernel']\n",
    "var_finetune=[x for x in module.variables for vv in v if vv in x.name] #get the weights we want to finetune.\n",
    "\n",
    "# optimizer & losses\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "loss_history = []\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # https://www.tensorflow.org/guide/eager\n",
    "    \n",
    "    # get encodings\n",
    "    question_embeddings = question_encoder(tf.constant(questions))['outputs']\n",
    "    response_embeddings = response_encoder(input=tf.constant(responses), \n",
    "                                           context=tf.constant(response_contexts))['outputs']\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\n",
    "    loss_value = loss(question_embeddings, response_embeddings)\n",
    "\n",
    "# record and apply loss gradients    \n",
    "loss_history.append(loss_value.numpy().mean())\n",
    "\n",
    "print(\"BEFORE BACKPROP\")\n",
    "print(var_finetune)\n",
    "print(\"\")\n",
    "\n",
    "print(\"...calculating and applying gradients...\")\n",
    "grads = tape.gradient(loss_value, var_finetune)\n",
    "optimizer.apply_gradients(zip(grads, var_finetune))\n",
    "print(\"\")\n",
    "\n",
    "print(\"AFTER BACKPROP\")\n",
    "print(var_finetune)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF2.Keras Sequential() implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras Sequential Implementation\n",
    "# module_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(hub_layer)\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer=keras.optimizers.RMSprop(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# question_embeddings = module.signatures['question_encoder'](\n",
    "#             tf.constant(questions))\n",
    "# response_embeddings = module.signatures['response_encoder'](\n",
    "#         input=tf.constant(responses),\n",
    "#         context=tf.constant(response_contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Azure SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"poodbc drivers\")\n",
    "print(pyodbc.drivers())\n",
    "\n",
    "odbc_str = \"\" \n",
    "\n",
    "import pyodbc \n",
    "conn = pyodbc.connect(odbc_str)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT * FROM dbo.users')\n",
    "\n",
    "for row in cursor:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing tensorflow_addon 's triplet loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple responses w/0 context\n",
    "encoded_ques = gr.predict([\"How old are you?\"], \n",
    "                         type='query')\n",
    "\n",
    "# multiple responses w/0 context\n",
    "encoded_res1 = gr.predict([\"I am 20 years old.\"], #, \"I love apple cider\"], \n",
    "                         type='response')\n",
    "\n",
    "# multiple responses w/0 context\n",
    "encoded_res2 = gr.predict([ \"I love apple cider\"], \n",
    "                         type='response')\n",
    "\n",
    "APN = tf.stack([encoded_ques, encoded_res1, encoded_res2])\n",
    "\n",
    "tfa_triplet_cost = tfa.losses.TripletSemiHardLoss(margin = 0.5)\n",
    "# cost_value = tfa_triplet_cost([1], APN)\n",
    "\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "metric_learning.pairwise_distance(APN, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, embeddings = y_true, y_pred\n",
    "# Reshape label tensor to [batch_size, 1].\n",
    "lshape = tf.shape(labels)\n",
    "labels = tf.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "# Build pairwise squared distance matrix.\n",
    "pdist_matrix = metric_learning.pairwise_distance(embeddings, squared=True)\n",
    "# Build pairwise binary adjacency matrix.\n",
    "adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
    "# Invert so we can select negatives only.\n",
    "adjacency_not = tf.math.logical_not(adjacency)\n",
    "\n",
    "batch_size = tf.size(labels)\n",
    "\n",
    "# Compute the mask.\n",
    "pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
    "mask = tf.math.logical_and(\n",
    "    tf.tile(adjacency_not, [batch_size, 1]),\n",
    "    tf.math.greater(pdist_matrix_tile,\n",
    "                    tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n",
    "mask_final = tf.reshape(\n",
    "    tf.math.greater(\n",
    "        tf.math.reduce_sum(\n",
    "            tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True),\n",
    "        0.0), [batch_size, batch_size])\n",
    "mask_final = tf.transpose(mask_final)\n",
    "\n",
    "adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
    "mask = tf.cast(mask, dtype=tf.dtypes.float32)\n",
    "\n",
    "# negatives_outside: smallest D_an where D_an > D_ap.\n",
    "negatives_outside = tf.reshape(\n",
    "    _masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "negatives_outside = tf.transpose(negatives_outside)\n",
    "\n",
    "# negatives_inside: largest D_an.\n",
    "negatives_inside = tf.tile(\n",
    "    _masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "semi_hard_negatives = tf.where(mask_final, negatives_outside,\n",
    "                               negatives_inside)\n",
    "\n",
    "loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "mask_positives = tf.cast(\n",
    "    adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n",
    "        tf.ones([batch_size]))\n",
    "\n",
    "# In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "#   in semihard, they take all positive pairs except the diagonal.\n",
    "num_positives = tf.math.reduce_sum(mask_positives)\n",
    "\n",
    "triplet_loss = tf.math.truediv(\n",
    "    tf.math.reduce_sum(\n",
    "        tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)),\n",
    "    num_positives)\n",
    "\n",
    "return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# from src.model2 import GoldenRetriever\n",
    "\n",
    "print(\"import success\")\n",
    "\n",
    "# @st.cache(allow_output_mutation=True)\n",
    "# def init():\n",
    "#     retriever = GoldenRetriever()\n",
    "#     #retriever.restore('./google_use_nrf_pdpa_tuned/variables-0')\n",
    "#     retriever.load_csv_kb(path_to_kb='./data/pdpa.csv', cutoff=196, kb_name='pdpa')\n",
    "#     retriever.load_kb(path_to_kb='./data/aiap.txt', is_faq=True, kb_name='aiap')\n",
    "#     retriever.load_kb(path_to_kb='./data/resale_tnc.txt', kb_name='resale_tnc')\n",
    "#     # retriever.load_kb(path_to_kb='./data/fund_guide_tnc_full.txt', kb_name='nrf')\n",
    "#     return retriever\n",
    "\n",
    "gr = GoldenRetriever()\n",
    "gr.load_csv_kb(path_to_kb='../data/pdpa.csv', cutoff=196, kb_name='pdpa')\n",
    "gr.load_kb(path_to_kb='../data/aiap.txt', is_faq=True, kb_name='aiap')\n",
    "print(\"init success\")\n",
    "\n",
    "st.title('GoldenRetriever')\n",
    "st.header('This Information Retrieval demo allows you to query FAQs, T&Cs, or your own knowledge base in natural language.')\n",
    "st.markdown('View the source code [here](https://github.com/nickyeolk/info_retrieve)!')\n",
    "st.markdown('Visit our [community](https://makerspace.aisingapore.org/community/ai-makerspace/) and ask us a question!')\n",
    "kb_to_starqn = {'pdpa':\"Can an organization retain the physical NRIC?\",\n",
    "                'resale_tnc':\"How much is the option fee?\",\n",
    "                'aiap':\"Do I need to pay for the program?\",\n",
    "                # 'nrf':\"Can I vire from EOM into travel?\",\n",
    "                'raw_kb':\"What do you not love?\"}\n",
    "\n",
    "def format_func(kb_name):\n",
    "    namedicts={'pdpa':'PDPA',\n",
    "                'resale_tnc':'HDB Resale',\n",
    "                'aiap':'AIAP',\n",
    "                # 'nrf':'NRF',\n",
    "                'raw_kb':'Paste Raw Text'}\n",
    "    return namedicts[kb_name]\n",
    "kb = st.selectbox('Select Knowledge Base', options=['pdpa', 'resale_tnc', 'aiap', 'raw_kb'],\n",
    "                    format_func=format_func)\n",
    "if kb=='raw_kb':\n",
    "    kb_raw = st.text_area(label='Paste raw text (terms separated by empty line)', \n",
    "                        value=\"\"\"I love my chew toy!\\n\\nI hate Mondays.\\n\"\"\")\n",
    "top_k = st.radio('Number of Results', options=[1,2,3], index=2)\n",
    "data = st.text_input(label='Input query here', value=kb_to_starqn[kb])\n",
    "if st.button('Fetch') or (data != kb_to_starqn[kb]): #So the answer will not appear right away\n",
    "    if kb=='raw_kb':\n",
    "        gr.load_kb(raw_text=kb_raw, kb_name='raw_kb')\n",
    "    prediction, scores = gr.make_query(data, top_k=int(top_k), kb_name=kb)\n",
    "    qn_string=\"\"\"<h3><text>Question: </text>{}</h3>\"\"\".format(data)\n",
    "    st.markdown(qn_string, unsafe_allow_html=True)\n",
    "\n",
    "    for ansnum, result in enumerate(prediction):\n",
    "        anshead_string = \"\"\"<h3><text>Answer {}</text></h3>\"\"\".format(ansnum+1)\n",
    "        st.markdown(anshead_string, unsafe_allow_html=True)\n",
    "        reply_string=\"\"\"<table>\"\"\"\n",
    "        lines = [line for line in result.split('\\n') if line]\n",
    "        for line in lines:\n",
    "            reply_string += \"\"\"<tr>\"\"\"\n",
    "            tabledatas = line.split(';;')\n",
    "            for tabledata in tabledatas:\n",
    "                if len(tabledatas)>1:\n",
    "                    line_string = \"\"\"<td>{}</td>\"\"\".format(tabledata)\n",
    "                else:\n",
    "                    line_string = \"\"\"<td colspan=42>{}</td>\"\"\".format(tabledata)\n",
    "                reply_string += line_string\n",
    "            reply_string += \"\"\"</tr>\"\"\"\n",
    "        reply_string+=\"\"\"</table><br>\"\"\"\n",
    "        st.markdown(reply_string, unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\n",
    "\"\"\"\n",
    "<details><summary>Sample sentences</summary>\n",
    "<strong>PDPA</strong>\n",
    "<p>How long can an organisation retain its customers' personal data?</p>\n",
    "<strong>HDB resale terms and conditions</strong>\n",
    "<p>Do I need to pay back CPF?</p>\n",
    "<strong>AIAP</strong>\n",
    "<p>What will be covered during the program?</p>\n",
    "<strong>Raw text </strong><a href=\"https://www.straitstimes.com/asia/east-asia/china-wants-centralised-digital-currency-after-bitcoin-crackdown\" target=\"_blank\">China Digital Currency</a><i> (Select all, copy, and paste into raw text box)</i>\n",
    "<p>Which electronic payment gateways support the currency?</p>\n",
    "</details>\"\"\"\n",
    ", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
