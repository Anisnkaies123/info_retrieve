{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting GoldenRetriever to TF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3  \n",
    "Official code sample for TF2  \n",
    "\n",
    "Transfer learning in TF-hub w TF1.15  \n",
    "https://colab.research.google.com/github/tensorflow/hub/blob/master/docs/tutorials/text_classification_with_tf_hub.ipynb#scrollTo=6OPyVxHuiTEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40884   , 0.08877401]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorflow_text\n",
    "# !pip install --upgrade tensorflow-hub\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "\n",
    "module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "\n",
    "question_embeddings = module.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "response_embeddings = module.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "\n",
    "np.inner(question_embeddings['outputs'], response_embeddings['outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.keras_layer.KerasLayer"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Keras Sequential Implementation\n",
    "# module_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(hub_layer)\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer=keras.optimizers.RMSprop(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# question_embeddings = module.signatures['question_encoder'](\n",
    "#             tf.constant(questions))\n",
    "# response_embeddings = module.signatures['response_encoder'](\n",
    "#         input=tf.constant(responses),\n",
    "#         context=tf.constant(response_contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning USE w GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "BEFORE BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00305502,  0.00504641,  0.01790689, ..., -0.02388328,\n",
      "         0.03720526,  0.04548807],\n",
      "       [ 0.04789947, -0.02582268,  0.08293641, ...,  0.0698828 ,\n",
      "        -0.04037469, -0.02779369],\n",
      "       [-0.05143448, -0.06723368,  0.02879738, ..., -0.04495105,\n",
      "        -0.04067428, -0.01053122]], dtype=float32)>]\n",
      "calculating and applying gradients\n",
      "AFTER BACKPROP\n",
      "[<tf.Variable 'QA/Final/Response_tuning/ResidualHidden_1/dense/kernel:0' shape=(320, 512) dtype=float32, numpy=\n",
      "array([[ 0.11290824, -0.007661  ,  0.13388894, ..., -0.03849068,\n",
      "        -0.05095735, -0.06322648],\n",
      "       [-0.02622743, -0.02499395, -0.01445046, ..., -0.10326502,\n",
      "         0.00695672, -0.17296325],\n",
      "       [-0.02357727, -0.08032651, -0.04250011, ..., -0.04690072,\n",
      "         0.01988911, -0.01170817],\n",
      "       ...,\n",
      "       [-0.00206157,  0.00408014,  0.01691419, ..., -0.02288993,\n",
      "         0.03819802,  0.04456301],\n",
      "       [ 0.04889183, -0.02678634,  0.08194487, ...,  0.07087511,\n",
      "        -0.03938333, -0.02870969],\n",
      "       [-0.05051216, -0.06792539,  0.02788324, ..., -0.04402984,\n",
      "        -0.03975946, -0.01102271]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "\n",
    "# load the module: v3 does not support finetuning? but v2 is fine\n",
    "module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/2')\n",
    "\n",
    "# get trainable layers\n",
    "#v=['QA/Final/Response_tuning/ResidualHidden_1/AdjustDepth/projection/kernel']\n",
    "v=['QA/Final/Response_tuning/ResidualHidden_1/dense/kernel']\n",
    "var_finetune=[x for x in module.variables for vv in v if vv in x.name] #get the weights we want to finetune.\n",
    "\n",
    "# optimizer & losses\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "loss_history = []\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # https://www.tensorflow.org/guide/eager\n",
    "    \n",
    "    # get encodings\n",
    "    question_embeddings = module.signatures['question_encoder'](tf.constant(questions))['outputs']\n",
    "    response_embeddings = module.signatures['response_encoder'](input=tf.constant(responses), \n",
    "                                                                context=tf.constant(response_contexts))['outputs']\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\n",
    "    loss_value = loss(question_embeddings, response_embeddings)\n",
    "\n",
    "# record and apply loss gradients    \n",
    "loss_history.append(loss_value.numpy().mean())\n",
    "\n",
    "print(\"BEFORE BACKPROP\")\n",
    "print(var_finetune)\n",
    "print(\"\")\n",
    "\n",
    "print(\"...calculating and applying gradients...\")\n",
    "grads = tape.gradient(loss_value, var_finetune)\n",
    "optimizer.apply_gradients(zip(grads, var_finetune))\n",
    "print(\"\")\n",
    "\n",
    "print(\"AFTER BACKPROP\")\n",
    "print(var_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few values in the array has changed, indicating that there is successful tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tensorboard/graphs\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO list:\n",
    " <font color=green>\n",
    "1. Init function\n",
    "2. Predict\n",
    "3. make_query   \n",
    "    \n",
    "</font>\n",
    "\n",
    "4. Finetune \n",
    "5. Contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "# import os\n",
    "# os.listdir('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "#from metric_learning import triplet_loss, contrastive_loss\n",
    "#from tensorflow.train import Saver\n",
    "from utils import split_txt, read_txt, clean_txt, read_kb_csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoldenRetriever:\n",
    "    \"\"\"GoldenRetriever model for information retrieval prediction and finetuning.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr: Learning rate (default 0.6)\n",
    "    loss: loss function to use. Options are 'cosine'(default), 'contrastive', or 'triplet' which is a triplet loss based on cosine distance.\n",
    "    margin: margin to be used if loss='triplet' (default 0.1)\n",
    "\n",
    "    Example:\n",
    "    >>> gr = GoldenRetriever()\n",
    "    >>> text_list = ['I love my chew toy!', 'I hate Mondays.']\n",
    "    >>> gr.load_kb(text_list=text_list)\n",
    "    >>> gr.make_query('what do you not love?', top_k=1)\n",
    "    ['I hate Mondays.']\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, lr=0.6, margin=0.3, loss='triplet'):\n",
    "        # self.v=['module/QA/Final/Response_tuning/ResidualHidden_1/dense/kernel','module/QA/Final/Response_tuning/ResidualHidden_0/dense/kernel', 'module/QA/Final/Response_tuning/ResidualHidden_1/AdjustDepth/projection/kernel']\n",
    "        self.v=['module/QA/Final/Response_tuning/ResidualHidden_1/AdjustDepth/projection/kernel']\n",
    "        self.lr = lr\n",
    "        self.margin = margin\n",
    "        self.loss = loss\n",
    "        self.vectorized_knowledge = {}\n",
    "        self.text = {}\n",
    "        self.questions = {}\n",
    "        \n",
    "        # init saved model\n",
    "        self.embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "        self.question_encoder = module.signatures['question_encoder']\n",
    "        self.response_encoder = module.signatures['response_encoder']\n",
    "        print('model initiated!')\n",
    "        \n",
    "        # to-confirm: question response placeholders really not needed?\n",
    "        # TODO: negative response encoder for contrastive loss\n",
    "        \n",
    "        \n",
    "    def predict(self, text, context=None, type='response'):\n",
    "        \"\"\"Return the tensor representing embedding of input text.\n",
    "        Type can be 'query' or 'response' \"\"\"\n",
    "        if type=='query':\n",
    "            return self.question_encoder(tf.constant([text]))['outputs']\n",
    "            # return self.session.run(self.question_embeddings, feed_dict={self.question:text})['outputs']\n",
    "        elif type=='response':\n",
    "            if not context:\n",
    "                context = text\n",
    "            return self.response_encoder(input=tf.constant(text),\n",
    "                                         context=tf.constant(context))['outputs']\n",
    "        else: print('Type of prediction not defined')\n",
    "        \n",
    "    def make_query(self, querystring, top_k=5, index=False, predict_type='query', kb_name='default_kb'):\n",
    "        \"\"\"Make a query against the stored vectorized knowledge. \n",
    "        Choose index=True to return sorted index of matches.\n",
    "        type can be 'query' or 'response' if you are comparing statements\n",
    "        \"\"\"\n",
    "        similarity_score=cosine_similarity(self.vectorized_knowledge[kb_name], self.predict([querystring], type=predict_type))\n",
    "        sortargs=np.flip(similarity_score.argsort(axis=0))\n",
    "        sortargs=[x[0] for x in sortargs]\n",
    "        sorted_ans=[self.text[kb_name][i] for i in sortargs]\n",
    "        if index:\n",
    "            return sorted_ans[:top_k], sortargs[:top_k]\n",
    "        return sorted_ans[:top_k], similarity_score[sortargs[:top_k]] \n",
    "        \n",
    "        \n",
    "    def load_kb(self, path_to_kb=None, text_list=None, question_list=None, \n",
    "                raw_text=None, is_faq=False, kb_name='default_kb'):\n",
    "        r\"\"\"Give either path to .txt document or list of clauses.\n",
    "        For text document, each clause is separated by 2 newlines ('\\\\n\\\\n')\"\"\"\n",
    "        if text_list:\n",
    "            self.text[kb_name] = text_list\n",
    "            if is_faq:\n",
    "                self.questions[kb_name] = question_list\n",
    "        elif path_to_kb:\n",
    "            if is_faq:\n",
    "                self.text[kb_name], self.questions[kb_name] = split_txt(read_txt(path_to_kb), is_faq)\n",
    "            else:\n",
    "                self.text[kb_name] = split_txt(read_txt(path_to_kb), is_faq)\n",
    "        elif raw_text:\n",
    "            delim = '\\n'\n",
    "            self.text[kb_name] = split_txt([front+delim for front in raw_text.split('\\n')])\n",
    "        else: raise NameError('invalid kb input!')\n",
    "        self.vectorized_knowledge[kb_name] = self.predict(clean_txt(self.text[kb_name]), type='response')\n",
    "        print('knowledge base lock and loaded!')\n",
    "        \n",
    "    def load_csv_kb(self, path_to_kb=None, kb_name='default_kb', meta_col='meta', answer_col='answer', \n",
    "                    query_col='question', answer_str_col='answer', cutoff=None):\n",
    "        self.text[kb_name], self.questions[kb_name] = read_kb_csv(path_to_kb, meta_col=meta_col, answer_col=answer_col, \n",
    "                            query_col=query_col, answer_str_col=answer_str_col, cutoff=None)\n",
    "        self.vectorized_knowledge[kb_name] = self.predict(clean_txt(self.text[kb_name]), type='response')\n",
    "        print('knowledge base (csv) lock and loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initiated!\n",
      "CPU times: user 18 s, sys: 769 ms, total: 18.8 s\n",
      "Wall time: 18.6 s\n",
      "knowledge base lock and loaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['We are looking for candidates who possess a keen interest in the area of machine learning and data science. We believe that candidates can come from any area of specialisation, and our requirements are as follow:\\ni)   Singaporean with a polytechnic diploma or university degree,\\nii) Proficient in Python or R and iii) Is able to implement Machine Learning Algorithms or have a background in Mathematics / Statistics / Computer Science. \\nBeyond that, demonstrated statistical fundamentals and programming ability will be helpful for the technical tests, but a keen learning attitude will be the most important to carry you through the programme. \\n',\n",
       "  'Candidates can expect to be equipped in some or all of the following skills: data modelling/tuning, data engineering, data product-related software engineering, cloud applications. It ranges between individuals, but candidates can be adequately prepared in fields of data science, engineering and consultancy\\n'],\n",
       " array([[0.3085595 ],\n",
       "        [0.28774393]], dtype=float32))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time gr = GoldenRetriever()\n",
    "\n",
    "# encode 1 question\n",
    "encoded_ques = gr.predict('How old are you?', \n",
    "                          type='query')\n",
    "\n",
    "# encode multiple questions\n",
    "encoded_ques = gr.predict(['How old are you?', 'What time is it?'], \n",
    "                          type='query')\n",
    "\n",
    "# one response w context\n",
    "encoded_res = gr.predict(\"I am 20 years old.\", \n",
    "                         context=\"I will be 21 next year.\", \n",
    "                         type='response')\n",
    "\n",
    "# multiple responses w/0 context\n",
    "encoded_res = gr.predict([\"I am 20 years old.\", \"I love apple cider\"], \n",
    "                         type='response')\n",
    "\n",
    "# load knowledge bases\n",
    "gr.load_kb(path_to_kb='../data/aiap.txt', is_faq=True, kb_name='aiap')\n",
    "# gr.load_kb(path_to_kb='./data/resale_tnc.txt', kb_name='resale_tnc')\n",
    "# gr.load_kb(path_to_kb='./data/fund_guide_tnc_full.txt', kb_name='nrf')\n",
    "# gr.load_csv_kb(path_to_kb='./data/pdpa.csv', cutoff=196, kb_name='pdpa')\n",
    "\n",
    "# make query\n",
    "gr.make_query('What kind of candidates are you looking for?', top_k=2, kb_name='aiap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
