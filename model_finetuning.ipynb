{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC of model finetuning using AISG FAQ data\n",
    "Note: this is not a rigorous proof of the finetuning abilities, it merely shows that we are able to overfit to one dataset. The main purpose is to build and test the finetuning code using tensorflow.\n",
    "Tested with just minimizing cosine loss. This kind of works, but having a triplet loss might be better.\n",
    "Triplet takes many more epochs before it gets the same accuracy as just minimizing cosine loss, but I think it should be more robust. Will have to try both on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from src.utils import read_txt, split_txt, aiap_qna_quickscore\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1002 03:28:03.795668 140102930614016 deprecation.py:323] From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1002 03:28:19.995347 140102930614016 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1002 03:28:27.000113 140102930614016 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1002 03:28:34.523622 140102930614016 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:331: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1002 03:28:36.135310 140102930614016 deprecation.py:323] From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:331: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1002 03:28:36.214597 140102930614016 deprecation.py:323] From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from src.model import QnaEncoderModel\n",
    "model=QnaEncoderModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.metric_learning import triplet_loss\n",
    "# import tf_sentencepiece\n",
    "# class QnaEncoderModel:\n",
    "#     def __init__(self, lr=0.6, margin=0.1, loss='triplet'):\n",
    "#         '''loss can be 'triplet'(default), or 'cosine'. '''\n",
    "#         self.v=v=['module/QA/Final/Response_tuning/ResidualHidden_1/dense/kernel','module/QA/Final/Response_tuning/ResidualHidden_0/dense/kernel', 'module/QA/Final/Response_tuning/ResidualHidden_1/AdjustDepth/projection/kernel']\n",
    "#         self.lr = lr\n",
    "#         self.margin = margin\n",
    "#         self.loss = loss\n",
    "#         # Set up graph.\n",
    "#         tf.reset_default_graph() # finetune\n",
    "#         g = tf.Graph()\n",
    "#         with g.as_default():\n",
    "#             self.embed = hub.Module(\"./google_use_qa\", trainable=True)\n",
    "#             # put placeholders\n",
    "#             self.question = tf.placeholder(dtype=tf.string, shape=[None])  # question\n",
    "#             self.response = tf.placeholder(dtype=tf.string, shape=[None])  # response\n",
    "#             self.response_context = tf.placeholder(dtype=tf.string, shape=[None])  # response context\n",
    "#             self.neg_response = tf.placeholder(dtype=tf.string, shape=[None])  # response\n",
    "#             self.neg_response_context = tf.placeholder(dtype=tf.string, shape=[None])  # response context\n",
    "#             self.label = tf.placeholder(tf.int32, [None], name='label')\n",
    "            \n",
    "#             self.question_embeddings = self.embed(\n",
    "#             dict(input=self.question),\n",
    "#             signature=\"question_encoder\", as_dict=True)\n",
    "\n",
    "#             self.response_embeddings = self.embed(\n",
    "#             dict(input=self.response,\n",
    "#                 context=self.response_context),\n",
    "#             signature=\"response_encoder\", as_dict=True)\n",
    "\n",
    "#             self.neg_response_embeddings = self.embed(\n",
    "#             dict(input=self.neg_response,\n",
    "#                 context=self.neg_response_context),\n",
    "#             signature=\"response_encoder\", as_dict=True)\n",
    "\n",
    "#             init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "\n",
    "#             # finetune\n",
    "#             # tf 1.13 does not have contrastive loss. Might have to self-implement.\n",
    "#             if self.loss=='triplet':\n",
    "#                 self.cost = triplet_loss(self.question_embeddings['outputs'], self.response_embeddings['outputs'], self.neg_response_embeddings['outputs'], margin=self.margin)\n",
    "#             elif self.loss=='cosine':\n",
    "#                 self.cost = tf.losses.cosine_distance(self.question_embeddings['outputs'], self.response_embeddings['outputs'], axis=1)\n",
    "#             else: raise Exception('invalid loss selected. Choose either triplet or cosine.')\n",
    "#             opt = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
    "#             var_finetune=[x for x in self.embed.variables for vv in self.v if vv in x.name] #get the weights we want to finetune.\n",
    "#             self.opt_op = opt.minimize(self.cost, var_list=var_finetune)\n",
    "# #         g.finalize()\n",
    "\n",
    "#         # Initialize session.\n",
    "#         self.session = tf.Session(graph=g, config=tf.ConfigProto(log_device_placement=True))\n",
    "#         self.session.run(init_op)\n",
    "    \n",
    "#     def predict(self, text, context=None, type='response'):\n",
    "#         if type=='query':\n",
    "#             return self.session.run(self.question_embeddings, feed_dict={self.question:text})['outputs']\n",
    "#         elif type=='response':\n",
    "#             if not context:\n",
    "#                 context = text\n",
    "#             return self.session.run(self.response_embeddings, feed_dict={\n",
    "#             self.response:text,\n",
    "#             self.response_context:context\n",
    "#             })['outputs']\n",
    "#         else: print('Type of prediction not defined')\n",
    "\n",
    "#     def finetune(self, question, answer, context, neg_answer=None, neg_answer_context=None):\n",
    "#         current_loss = self.session.run(self.cost, feed_dict={\n",
    "#             self.question:question,\n",
    "#             self.response:answer,\n",
    "#             self.response_context:context,\n",
    "# #             self.neg_response:neg_answer,\n",
    "# #             self.neg_response_context:neg_answer_context\n",
    "#             })\n",
    "#         self.session.run(self.opt_op, feed_dict={\n",
    "#             self.question:question,\n",
    "#             self.response:answer,\n",
    "#             self.response_context:context,\n",
    "# #             self.neg_response:neg_answer,\n",
    "# #             self.neg_response_context:neg_answer_context\n",
    "#             })\n",
    "#         return current_loss\n",
    "        \n",
    "#     def export(self):\n",
    "#         saver=Saver(self.embed.variables)\n",
    "#         saver.save(self.session, 'fine_tuned', global_step=0)\n",
    "    \n",
    "#     def restore(self, savepath):\n",
    "#         saver=Saver(self.embed.variables)\n",
    "#         saver.restore(self.session, savepath)\n",
    "\n",
    "#     def close(self):\n",
    "#         self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiap_qa, aiap_context = split_txt(read_txt('./data/aiap.txt'), qa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q1. WHAT SORT OF CANDIDATES ARE YOU LOOKING FOR?', 'Q2. WHAT WILL BE COVERED IN THE PROGRAMME?', 'Q3. DO I HAVE TO PAY?', 'Q4. WHAT IS THE OUTCOME OF THIS PROGRAMME?', 'Q5. WILL I GET A JOB AFTER THE PROGRAMME?', 'Q6. WILL THERE BE A NEXT ROUND OF APPLICATION?', 'Q7. DO I HAVE TO GIVE UP MY CURRENT JOB TO JOIN THE  PROGRAMME?', 'Q8. WHAT IF I DECIDED TO DROP OUT OF THE PROGRAMME?', 'Q9. WILL I GET SOME SORT OF CERTIFICATE TO SHOW THAT I PARTICIPATED IN THE AIAP?', 'Q10. WHAT ARE THE AI JOBS AND ROLES I CAN CONSIDER AFTER COMPLETING THE TRAINING PROGRAMME?']\n"
     ]
    }
   ],
   "source": [
    "print(aiap_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01127431,  0.04894139, -0.06490619, ...,  0.09330796,\n",
       "         0.01958818, -0.02537909],\n",
       "       [ 0.04272489,  0.0623336 ,  0.02158852, ...,  0.06216078,\n",
       "         0.00950297, -0.03111232],\n",
       "       [-0.05566482,  0.02913201,  0.02554237, ...,  0.04592849,\n",
       "         0.00670884, -0.07619023],\n",
       "       ...,\n",
       "       [-0.01923553, -0.06181136,  0.0150568 , ...,  0.0781768 ,\n",
       "         0.02878742, -0.07518731],\n",
       "       [-0.01476899, -0.05544961,  0.09171879, ...,  0.0126055 ,\n",
       "        -0.02055403, -0.05686105],\n",
       "       [ 0.05459761,  0.03379358,  0.01586365, ...,  0.03275432,\n",
       "         0.04136326,  0.03180375]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(aiap_qa, type='query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5\n",
      "1 0.6\n",
      "2 0.6\n",
      "3 0.8\n",
      "4 0.9\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    print(k, aiap_qna_quickscore(aiap_context, model.predict(aiap_qa, type='response'), aiap_qa, model, k+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1002 03:29:22.082569 140102930614016 deprecation.py:323] From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /data/home/lik/info_retrieve/google_use_qa_tuned/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1002 03:29:22.084938 140102930614016 saver.py:1270] Restoring parameters from /data/home/lik/info_retrieve/google_use_qa_tuned/variables\n"
     ]
    }
   ],
   "source": [
    "model.restore('/data/home/lik/info_retrieve/google_use_qa_tuned/variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8\n",
      "1 0.8\n",
      "2 0.9\n",
      "3 1.0\n",
      "4 1.0\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    print(k, aiap_qna_quickscore(aiap_context, model.predict(aiap_qa, type='response'), aiap_qa, model, k+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q5. WILL I GET A JOB AFTER THE PROGRAMME?',\n",
       " 'Q6. WILL THERE BE A NEXT ROUND OF APPLICATION?',\n",
       " 'Q8. WHAT IF I DECIDED TO DROP OUT OF THE PROGRAMME?',\n",
       " 'Q2. WHAT WILL BE COVERED IN THE PROGRAMME?',\n",
       " 'Q1. WHAT SORT OF CANDIDATES ARE YOU LOOKING FOR?',\n",
       " 'Q3. DO I HAVE TO PAY?',\n",
       " 'Q9. WILL I GET SOME SORT OF CERTIFICATE TO SHOW THAT I PARTICIPATED IN THE AIAP?',\n",
       " 'Q7. DO I HAVE TO GIVE UP MY CURRENT JOB TO JOIN THE  PROGRAMME?',\n",
       " 'Q4. WHAT IS THE OUTCOME OF THIS PROGRAMME?',\n",
       " 'Q10. WHAT ARE THE AI JOBS AND ROLES I CAN CONSIDER AFTER COMPLETING THE TRAINING PROGRAMME?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "aiap_neg = aiap_context.copy()\n",
    "random.shuffle(aiap_neg)\n",
    "aiap_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    for answer, question, neg_qn in zip(aiap_qa, aiap_context, aiap_neg):\n",
    "#         print(answer, question)\n",
    "        model.finetune([question], [answer], [answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8\n",
      "1 0.8\n",
      "2 0.9\n",
      "3 1.0\n",
      "4 1.0\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    print(k, aiap_qna_quickscore(aiap_context, model.predict(aiap_qa), aiap_qa, model, k+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=QnaEncoderModel(loss='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epoch 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning on toy dataset works! Now to try for insurance dataset\n",
    "This is more rigorous, cause I will be testing on unseen data.\n",
    "## First load the test framework for qna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import question_cleaner, display_qn_and_ans, aiap_qna, ranker, scorer\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:2000, removed:677, remainder:1323\n"
     ]
    }
   ],
   "source": [
    "datapath=Path('./data')\n",
    "df_query = pd.read_csv(datapath/'insuranceQA/V2/InsuranceQA.question.anslabel.raw.500.pool.solr.train.encoded', delimiter='\\t', header=None)\n",
    "df_doc = pd.read_csv(datapath/'insuranceQA/V2/InsuranceQA.label2answer.raw.encoded', delimiter='\\t', header=None)\n",
    "df_ind2word = pd.read_csv(datapath/'insuranceQA/V2/vocabulary', sep='\\t', header=None, quotechar='', quoting=3, keep_default_na=False)\n",
    "dict_ind2word = pd.Series(df_ind2word[1].values,index=df_ind2word[0].values).to_dict()\n",
    "\n",
    "df_test = pd.read_csv(datapath/'insuranceQA/V2/InsuranceQA.question.anslabel.raw.100.pool.solr.test.encoded', delimiter='\\t', header=None)\n",
    "df_test=question_cleaner(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:12889, removed:2498, remainder:10391\n"
     ]
    }
   ],
   "source": [
    "df_query=question_cleaner(df_query)\n",
    "df_doc=df_doc.set_index(0)\n",
    "\n",
    "def func(row):\n",
    "    kb=[int(xx) for xx in (row[3]).split(' ')]\n",
    "    gt = [int(xx) for xx in (row[2]).split(' ')]\n",
    "    return random.sample([xx for xx in kb if xx not in gt], 100)\n",
    "df_query['neg_samples']=df_query.apply(lambda x: func(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>neg_samples</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>life-insurance</td>\n",
       "      <td>idx_3019 idx_16371 idx_5499 idx_448 idx_136 id...</td>\n",
       "      <td>26354</td>\n",
       "      <td>13324 23395 7870 15080 27181 25052 26694 568 2...</td>\n",
       "      <td>[127, 21311, 22635, 11029, 4920, 2125, 6970, 1...</td>\n",
       "      <td>Can Creditors Take Life Insurance After Death?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0                                                  1      2  \\\n",
       "1  life-insurance  idx_3019 idx_16371 idx_5499 idx_448 idx_136 id...  26354   \n",
       "\n",
       "                                                   3  \\\n",
       "1  13324 23395 7870 15080 27181 25052 26694 568 2...   \n",
       "\n",
       "                                         neg_samples  \\\n",
       "1  [127, 21311, 22635, 11029, 4920, 2125, 6970, 1...   \n",
       "\n",
       "                                             text  \n",
       "1  Can Creditors Take Life Insurance After Death?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medicare-insurance</td>\n",
       "      <td>idx_2363 idx_467 idx_8080 idx_31 idx_9966 idx_...</td>\n",
       "      <td>9128</td>\n",
       "      <td>9128 13322 21601 21471 6442 5412 24861 23536 2...</td>\n",
       "      <td>Will Medicare Pay For Smoking Cessation?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0                                                  1  \\\n",
       "4  medicare-insurance  idx_2363 idx_467 idx_8080 idx_31 idx_9966 idx_...   \n",
       "\n",
       "      2                                                  3  \\\n",
       "4  9128  9128 13322 21601 21471 6442 5412 24861 23536 2...   \n",
       "\n",
       "                                       text  \n",
       "4  Will Medicare Pay For Smoking Cessation?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_...</td>\n",
       "      <td>Coverage follows the car. Example 1: If you we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   1  \\\n",
       "0                                                      \n",
       "1  idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_...   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1  Coverage follows the car. Example 1: If you we...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wordifier(tokes):\n",
    "    return ' '.join([dict_ind2word[ind] for ind in tokes.strip().split(' ')])\n",
    "df_doc['text']=df_doc.apply(lambda x: wordifier(x[1]), axis=1)\n",
    "df_query['text']=df_query.apply(lambda x: wordifier(x[1]), axis=1)\n",
    "df_test['text']=df_test.apply(lambda x: wordifier(x[1]), axis=1)\n",
    "display(df_query.head(1))\n",
    "display(df_test.head(1))\n",
    "display(df_doc.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hard_wrong(question, all_wrong_ans, model, k=1):\n",
    "    wrong_answer_array=model.predict(all_wrong_ans, type='response')\n",
    "    sorted_ans, sortargs, similarity_score=aiap_qna(question, wrong_answer_array, all_wrong_ans, model, k=50)\n",
    "    return sorted_ans[0]\n",
    "\n",
    "def generate_triplets(df_query):\n",
    "    answers=[]\n",
    "    questions=[]\n",
    "    wrong_answers=[]\n",
    "    for ii, row in df_query.iterrows():\n",
    "        all_correct_ans = [int(xx) for xx in row[2].split(' ')]\n",
    "        all_wrong_ans = df_doc.loc[row['neg_samples'], 'text'].tolist()\n",
    "        for ans in all_correct_ans:\n",
    "            answers.append(df_doc.loc[ans, 'text'])\n",
    "            questions.append(row['text'])\n",
    "            wrong_answers.append(get_hard_wrong(row['text'], all_wrong_ans, model))\n",
    "    return answers, questions, wrong_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers, questions, wrong_answers=generate_triplets(df_query)\n",
    "# with open(\"./tmp_trainset.pickle\", \"wb\") as f:\n",
    "#     pickle.dump((answers,questions,wrong_answers), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./tmp_trainset.pickle\", \"rb\") as f:\n",
    "    answers,questions,wrong_answers = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(wrong_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.024368823\n",
      "100 0.0042471886\n",
      "200 0.010198474\n",
      "300 0.0\n",
      "400 0.0\n",
      "500 0.0\n",
      "600 0.0\n",
      "700 0.0\n",
      "800 0.012136817\n",
      "900 0.013827622\n",
      "1000 0.0\n",
      "1100 0.0052496195\n",
      "1200 0.0049781203\n",
      "1300 0.0025287867\n",
      "1400 0.0\n",
      "1500 0.0\n",
      "1600 0.0\n",
      "1700 0.0\n",
      "1800 0.0\n",
      "1900 0.0\n",
      "2000 0.0\n",
      "2100 0.0\n",
      "2200 0.0\n",
      "2300 0.0\n",
      "2400 0.0\n",
      "2500 0.001412332\n",
      "2600 0.0\n",
      "2700 0.0\n",
      "2800 0.0\n",
      "2900 0.0\n",
      "3000 0.0\n",
      "3100 0.0\n",
      "3200 0.0009291172\n",
      "3300 0.0\n",
      "3400 0.0\n",
      "3500 0.0\n",
      "3600 0.0\n",
      "3700 0.0\n",
      "3800 0.0\n",
      "3900 0.0\n",
      "4000 0.0\n",
      "4100 0.0\n",
      "4200 0.0\n",
      "4300 0.0\n",
      "4400 0.0\n",
      "4500 0.0\n",
      "4600 0.0\n",
      "4700 0.0\n",
      "4800 0.0\n",
      "4900 0.0\n",
      "5000 0.0\n",
      "5100 0.0\n",
      "5200 0.0\n",
      "5300 0.0\n",
      "5400 0.0\n",
      "5500 0.0\n",
      "5600 0.0\n",
      "5700 0.0\n",
      "5800 0.0\n",
      "5900 0.0\n",
      "6000 0.0\n",
      "6100 0.0\n",
      "6200 0.0\n",
      "6300 0.0\n",
      "6400 0.0\n",
      "6500 0.0\n",
      "6600 0.0\n",
      "6700 0.0\n",
      "6800 0.0\n",
      "6900 0.0\n",
      "7000 0.0\n",
      "7100 0.0\n",
      "7200 0.0\n",
      "7300 0.0\n",
      "7400 0.0\n",
      "7500 0.0\n",
      "7600 0.0\n",
      "7700 0.0\n",
      "7800 0.0\n",
      "7900 0.0\n",
      "8000 0.0\n",
      "8100 0.0\n",
      "8200 0.0\n",
      "8300 0.0\n",
      "8400 0.0\n",
      "8500 0.0\n",
      "8600 0.0\n",
      "8700 0.0\n",
      "8800 0.0\n",
      "8900 0.0\n",
      "9000 0.0\n",
      "9100 0.0\n",
      "9200 0.0\n",
      "9300 0.0\n",
      "9400 0.0\n",
      "9500 0.0\n",
      "9600 0.0\n",
      "9700 0.0\n",
      "9800 0.0\n",
      "9900 0.0\n",
      "10000 0.0\n",
      "10100 0.0\n",
      "10200 0.0\n",
      "10300 0.0\n",
      "10400 0.0\n",
      "10500 0.0\n",
      "10600 0.0\n",
      "10700 0.0\n",
      "10800 0.0\n",
      "10900 0.0\n",
      "11000 0.0\n",
      "11100 0.0\n",
      "11200 0.0\n",
      "11300 0.0\n",
      "11400 0.0\n",
      "11500 0.0\n",
      "11600 0.0\n",
      "11700 0.0\n",
      "11800 0.0\n",
      "11900 0.0\n",
      "12000 0.0\n",
      "12100 0.0\n",
      "12200 0.0\n",
      "12300 0.0\n",
      "12400 0.0\n",
      "12500 0.0\n",
      "12600 0.0\n",
      "12700 0.0\n",
      "12800 0.0\n",
      "12900 0.0\n",
      "13000 0.0\n",
      "13100 0.0\n",
      "13200 0.0\n",
      "13300 0.0\n",
      "13400 0.0\n",
      "13500 0.0\n",
      "13600 0.0\n",
      "13700 0.0\n",
      "13800 0.0\n",
      "13900 0.0\n",
      "14000 0.0\n",
      "14100 0.0\n",
      "14200 0.0\n",
      "14300 0.0\n",
      "14400 0.0\n",
      "14500 0.0\n",
      "14600 0.0\n",
      "14700 0.0\n",
      "14800 0.0\n",
      "14900 0.0\n",
      "15000 0.0\n",
      "15100 0.0\n",
      "15200 0.0\n",
      "15300 0.0\n",
      "15400 0.0\n",
      "15500 0.0\n",
      "15600 0.0\n",
      "15700 0.0\n",
      "15800 0.0\n",
      "15900 0.0\n",
      "16000 0.0\n",
      "16100 0.0\n",
      "16200 0.0\n",
      "16300 0.0\n",
      "16400 0.0\n",
      "16500 0.0\n",
      "16600 0.0\n",
      "16700 0.0\n",
      "16800 0.0\n",
      "16900 0.0\n",
      "17000 0.0\n",
      "17100 0.0\n",
      "17200 0.0\n",
      "17300 0.0\n",
      "17400 0.0\n",
      "17500 0.0\n",
      "17600 0.0037795305\n",
      "17700 0.0\n",
      "17800 0.0\n",
      "17900 0.0\n",
      "18000 0.0\n",
      "18100 0.013442099\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "for _ in range(1):\n",
    "    for ii in range(0, len(answers), batch_size):\n",
    "        (kb, query, neg_query) = (questions[ii:ii+batch_size], answers[ii:ii+batch_size], wrong_answers[ii:ii+batch_size])\n",
    "        current_loss = model.finetune(kb, query, query, neg_query)\n",
    "        print(ii, current_loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions vectorized!\n",
      "Score @1: 0.3915\n",
      "Score @2: 0.5200\n",
      "Score @3: 0.5949\n",
      "Score @4: 0.6493\n",
      "Score @5: 0.6984\n"
     ]
    }
   ],
   "source": [
    "question_vectors = model.predict(df_test['text'].tolist(), type='query')\n",
    "print('questions vectorized!')\n",
    "predictions, gts = ranker(model, question_vectors, df_test, df_doc)\n",
    "for k in range(5):\n",
    "    print('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score @1: 0.3870\n",
      "Score @2: 0.5193\n",
      "Score @3: 0.5896\n",
      "Score @4: 0.6478\n",
      "Score @5: 0.6977\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    print('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained\n",
    "Score @1: 0.3870  \n",
    "Score @2: 0.5193  \n",
    "Score @3: 0.5896  \n",
    "Score @4: 0.6478  \n",
    "Score @5: 0.6977  \n",
    "\n",
    "## Using cosine loss (1 epoch, lr=0.1)\n",
    "Score @1: 0.4067  \n",
    "Score @2: 0.5427  \n",
    "Score @3: 0.6153  \n",
    "Score @4: 0.6667  \n",
    "Score @5: 0.7067  \n",
    "\n",
    "## Using triplet loss (1 epoch, lr=0.1, margin=0.1) no shuffling of data\n",
    "Score @1: 0.3915  \n",
    "Score @2: 0.5200  \n",
    "Score @3: 0.5949  \n",
    "Score @4: 0.6493  \n",
    "Score @5: 0.6984  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
